---
title: "PracticalMachineLearningAssignment"
author: "Lynda Young"
date: "13 February 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```
# Assignment Objective and Summary

Data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants performing a weightlifting exercise is used to predict the way in which the participant performed the exercise. This exercise was done in 5 different ways -  classified A (correctly) and B thru E which were different versions of the incorrect movement.

The final classification model adopted was a random forest model using 54 predictors.  The out of sample error measured on validation data withheld from the original training set of data was 0.57% (accuracy of 99.43%).  This can be compared to an estimated out of sample error of 1.2% (accuracy of 98.8%) determined from the cross validation of the model performed using train.  

#1. Obtain and Explore and Manipulate Data
### 1.1 Obtain data
```{r inital}
# comment out those lines that dont need to be run again and / or take up a lot of room
#library(downloader)
#fileurl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#fileurl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
setwd("C:/Users/Lynda Young/Documents/Coursera/Practical Machine Learning")
#download.file(fileurl1,"pml-training.csv")
#download.file(fileurl2,"pml-testing.csv")

pml_training <- read.csv(file = "pml-training.csv",na.strings = c("NA","") )
pml_testing <- read.csv(file = "pml-testing.csv",na.strings = c("NA",""))

dim(pml_training)
#table(pml_training$classe)
#summary(pml_training)
```
Training data has 19,622 obs of 160 variables. An overview of the data from the summary commands shows that the are a number of variables where the majority of values are missing.  This needs to be addressed prior to modelling.  Particularly for a random forest model which can't deal with missing values.  This is dealt with in the code below.  

### 1.2 Identifying and allowing for initial missing data
```{r missing data}
# explore degree of any misisng data
# do for both pml_training and pml_testing to enable easier further processing on both data sets later 
# although both pml_testing and pml_training have the same # of variables, two variables are different.  
# Testing has a problem_id and no classe variable. Need to make names the same so can row bind the data
# 
library(dplyr)
pml_training$problem_id <-0
pml_testing$classe <- ""
total_data <- rbind(pml_training, pml_testing)
missing_proportion <- sapply(total_data, function(x) sum(is.na(x))/nrow(total_data))
table(missing_proportion)
low_missing_proportion <- missing_proportion <0.5
total_data <- total_data[,low_missing_proportion]

# remove a few other variables not to be used in analysis 
# data is ordered and split back into test and train
total_data <- select(total_data,-X,-cvtd_timestamp,-raw_timestamp_part_1,-raw_timestamp_part_2,-num_window,-new_window)
training_data <- total_data[1:19622,]
testing_data <- total_data[19623:19642,]
```
It can be seen that 100 of the 160 variables have more than 97% of values missing. If there was only a small % of the values missing it may have been possible to impute values. With such a large % of data missing the appropriate treatment is to exclude these variables from the data and create a new training data set called "training_data" that has 60 variables. 

Prior to charting the data some additional variables that are not needed are removed from the training data.  These are: 

        - X removed as it is the row number and comes up as a predictor because the data is ordered 
        - cvtd_timestamp as it appears to be the day and time of the exercise which is not relevant 
        - raw_timestamp_part_1 & num_window  
 
These last 2 variables are key predictors in determining which of the classes the exercise corresponds to.  However as they are time-related data it is really only classifying the exercises based on the order in which they were done and grouping all readings in a particular time-bracket which is not really the object of the exercise. Even though the assignment stated you could use "any of the other variables to predict with", I have limited my model to data from the accelerometers and user_name. Including timestamp_part_1 and num_window data in the random forest model increased its accuracy to 99.9% on the validation set !  

### 1.3 Charting predictor values and consideration of required data pre-processing 
Now to explore these variables using pairs charts. Note only one of the pairs charts is shown in this document due to limits on size. 
```{r explore data using pairs charts}

#pairs(training_data[,2:5],col=training_data$user_name)
pairs(training_data[,6:10],col=training_data$user_name)
#pairs(training_data[,11:15],col=training_data$user_name)
#pairs(training_data[,16:20],col=training_data$user_name)
#pairs(training_data[,21:25],col=training_data$user_name)
#pairs(training_data[,26:30],col=training_data$user_name)
#pairs(training_data[-5373,31:35],col=training_data$user_name)
#pairs(training_data[,36:40],col=training_data$user_name)
#pairs(training_data[-5373,41:45],col=training_data$user_name)
#pairs(training_data[-5373,46:50],col=training_data$user_name)
#pairs(training_data[-5373,51:55],col=training_data$user_name)

```
The colours in the charts indicate different participants. It can be seen that the distribution and values of many of the measurement variables differ by participant and suggest that user_name may be a key variable. (However later analysis shows this not to be the case).  Some of the charts also identified outlier values that are abnormally large and may be errors and for some participants certain accelerometer readings were found to be all zero.  

As it is proposed to use classifier models and not regression, the outliers are not expected to impact on the model and have not been adjusted for. For the same reason I have decided not to centre and scale the variables prior to processing, or reduce the size of some of the larger predictors by taking log values. 

Regarding the zero readings - user "jeremy" had zero readings for roll_arm, pitch_arm and yaw_arm and user "adelmo" had zero readings for roll_forearm, pitch_forearm and Yaw_forearm. I assumed these zero readings were most likely an error possibly due to the accelerometer on the arm/forearm not working for these participants and decided to use pre-processing to impute the missing values.   This involved setting the zero values to NA, imputing the missing values using knnimpute and predicting new values using the imputed object.  A chart of all the relevant values pre and post the imputation was created to observe the impact. (Charts not included to save space)
``` {R Impute missing (zero) values}
library(caret)
# plot of data before pre-processing
#pairs(training_data[,15:19],col=training_data$user_name)
# set zero values to NA but revert back to total data so it is done for both train and test
total_data$roll_arm[total_data$user_name == "jeremy"] <- NA
total_data$pitch_arm[total_data$user_name == "jeremy"] <- NA
total_data$yaw_arm[total_data$user_name == "jeremy"] <- NA
total_data$roll_forearm[total_data$user_name == "adelmo"] <- NA
total_data$pitch_forearm[total_data$user_name == "adelmo"] <- NA
total_data$yaw_forearm[total_data$user_name == "adelmo"] <- NA
# impute missing values and put back into total_data
impute_obj <- preProcess(total_data,method = "knnImpute")
imp_total_data <- predict(impute_obj,total_data)

total_data$roll_arm[total_data$user_name == "jeremy"] <- imp_total_data$roll_arm[imp_total_data$user_name == "jeremy"]
total_data$pitch_arm[total_data$user_name == "jeremy"] <- imp_total_data$pitch_arm[imp_total_data$user_name == "jeremy"]
total_data$yaw_arm[total_data$user_name == "jeremy"] <- imp_total_data$yaw_arm[imp_total_data$user_name == "jeremy"]
total_data$roll_forearm[total_data$user_name == "adelmo"] <- imp_total_data$roll_forearm[imp_total_data$user_name == "adelmo"]
total_data$pitch_forearm[total_data$user_name == "adelmo"] <- imp_total_data$pitch_forearm[imp_total_data$user_name == "adelmo"]
total_data$yaw_forearm[total_data$user_name == "adelmo"] <- imp_total_data$yaw_forearm[imp_total_data$user_name == "adelmo"]

#plot of data after pre-processing
#pairs(imp_total_data[,15:19],col=imp_training_data$user_name)
```

### 1.4 Examine the correlations of predictor variables
Using training data explore the correlations of numeric variables.

``` {r correlations of features}

numeric_vars <- sapply(total_data, function(x) class(x)!="factor")
library(corrplot)
correlations <- cor(total_data[,numeric_vars])
corrplot(correlations, method="circle", type="lower", sig.level = 0.005 , insig = "pch" , pch=1 , tl.cex = 0.5 )
high_correlations <- (abs(correlations[,])>=0.8)*correlations
#corrplot(high_correlations, method="circle", type="lower", sig.level = 0.005 , insig = "pch" , pch=1 ,tl.cex = 0.5)
```

13 variables have a high correlation (+/- 0.8) with at least one other variable.  This suggests that principal component analysis may be a useful approach to reduce the number of variables.  However once again this would be more important in the case of regression models.  Using PCA makes it difficult to interpret the key predictors and so on balance I have determined not to proceed with this approach.  

This may impact the identification of the level of importance of particular predictors in the model as the random forest importance measure assumes the predictors will be independent.  However I do not believe it will impact on the predictive capability of the model and obtaining the most accurate prediction is the main objective of this assignment and in this particular situation understanding predictor importance is a secondary consideration.

### 1.5 Establishing a training and validation data set
Create a data partition of 75% of the training data to train the model(s) on and 25% retained for out of sample validation.  Having the independent validation data is particularly important for the random forest model to ensure it has not over fitted the predictors using the values in the training data.   Train_final and validation are  the final training data set and validation data sets respectively. 

```{r Partition Data}
# split total_data back into train and test and remove problem_id from training data
training_data <- total_data[1:19622,]
testing_data <- total_data[19623:19642,]
training_data <- select(training_data,-problem_id)
# separate training_data into a train and validation set
set.seed(123)
training_data$classe <- factor(training_data$classe) # done to remove classe "" which has no values 
intrain <- createDataPartition(training_data$classe,p=0.75)[[1]]
train_final <- training_data[intrain,]
validation <- training_data[-intrain,]
```
#2. Model Training and Predictor selection
### 2.1 - Partition Tree (Rpart)

Initially a simple Rpart model was run to determine some of the key variables.  This was a poor model which resulted in only 50% accuracy on the validation data and did not predict any class D values.   It identified the 4 important predictors as: 
 - roll_belt
 - pitch_forearm
 - magnet_dumbbell
 - roll_forearm

From earlier charting I felt that user_name was an important distinguishing variable not picked up by Rpart for some reason and so proceeded to establish a rpart model and prediction for each separate participant using the code below.  This improved the accuracy to 65% which is still not great.
``` {R Rpart Model}
set.seed(123)
library(rattle)
weightlifters <- c("adelmo","carlitos","charles","eurico","jeremy","pedro")
model_rpart <- as.list(NULL)
validation$prediction_rpart <- factor(x = rep("A",times = 4904),levels =c("A","B","C","D","E"))   
for (i in (1:6)) {
 #       print(weightlifters[i])
        model_rpart[[i]] <- train(classe ~ . , data = train_final[train_final$user_name == weightlifters[i],],method = "rpart")
#        fancyRpartPlot(model_rpart[[i]]$finalModel)
        validation$prediction_rpart[validation$user_name == weightlifters[i]] <- predict(model_rpart[[i]],newdata = validation[validation$user_name == weightlifters[i],])
        rpart_accuracy <- confusionMatrix(validation$prediction_rpart[validation$user_name == weightlifters[i]],validation$classe[validation$user_name == weightlifters[i]]) }
  #      print(rpart_accuracy) }

# overall accuracy for all predictions from the 5 rpart models using validation data
rpart_accuracy <- confusionMatrix(validation$prediction_rpart,validation$classe)
rpart_accuracy
```
### 2.2 - Conditional Regresssion Tree (Ctree)
Next model tried was the Ctree model and a similar process to the RPart model followed. The initial simple model of Ctree across all the training data had an accuracy of 71%.  This was an improvement over the more complicated Rpart model by user_name.  Applying the user name approach to Ctree increased the accuracy to 93.7% on the validation data -  a significantly better model.

```{r CTREE}
set.seed(123)
model_ctree <- as.list(NULL)
validation$prediction_ctree <- factor(x = rep("A",times = 4904),levels =c("A","B","C","D","E"))  
for(i in(1:6)) {
#        print(weightlifters[i])
        model_ctree[[i]] <- train(classe~.,data = train_final[train_final$user_name == weightlifters[i],],method = "ctree")
                     #controls = ctree_control(mincriterion = .95,minsplit = 25,minbucket=10 
                                                         # ,maxdepth = 15,testtype = "Bonferroni"))
 #       plot(model_ctree[[i]]$finalModel,main=paste("Conditional Partition Tree for Weight Lifting Excercise -",weightlifters[i]),type = "simple")
        validation$prediction_ctree[validation$user_name == weightlifters[i]] <- predict(model_ctree[[i]],newdata = validation[validation$user_name == weightlifters[i],])
        accuracy <- confusionMatrix(validation$prediction_ctree[validation$user_name == weightlifters[i]],validation$classe[validation$user_name == weightlifters[i]]) }
   #     print(accuracy) }

# overall accuracy for all predictions from the 5 ctree models using validation data
ctree_accuracy <- confusionMatrix(validation$prediction_ctree,validation$classe)
ctree_accuracy
```
### 2.3 - Random Forest (RF)
Finally a Random Forrest model was run. I had tried this earlier however had problems with the run time and so had to investigate parallel processing to get it to run in a reasonable time in the end.

Initially, like the other models I ran a simple random forest across all the data.  However the results from this model were so good there was no need to create separate models for each user.  The estimated accuracy of this model on the cross-validation applied by train on the training data was 98.8% when the optional number of predictors randomly sampled was 29. I reduced the number of trees down from the default 500 to 100 as this was more than enough to otain a low error rate and improved the running time. 

The accuracy improved when measured on the validation data to  99.27%.  Suggesting a very good fit and no indication of over fitting.  This model was a considerable improvement over the ctree model.

```{r RF}
# for parallel procesing with random forest because large and computationally slow
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

# rf model ntree parameter determined from earlier running and set lower than default of 500 to help with processing with no impact on results.
set.seed(123)
model_rf <- train(classe~.,data = train_final,method = "rf", ntree = 100, do.trace=FALSE,
                  trControl =trainControl(allowParallel = TRUE,seeds= NA)) 
stopCluster(cluster) # return to normal processing
registerDoSEQ()

# rf output
model_rf
plot(model_rf)
plot(model_rf$finalModel)
rf_imp <-( varImp(model_rf,scale=FALSE) )
plot(rf_imp)
validation$prediction_rf <- predict(model_rf,newdata = validation)
rf_accuracy <- confusionMatrix(validation$prediction_rf,validation$classe)
rf_accuracy
```
# 3 - Selected Model and Predictions for test data using Random Forest Model

I tried an ensemble of the 3 model results by fitting a random forest to the predictor outcomes from each of the three models using the validation data.  However the accuracy of that ensemble model was no better than that of the random forest alone.

Therefore I selected the random forest model using all predictor variables contained in train_final. (I could have excluded those predictors ranked as not important but didn't go back and do this.)

The RF model was then used on the test data set to predict the classes as follows.
``` {r Prediction on test}
test_pred <- predict(model_rf,newdata = pml_testing)
print(test_pred)
```
